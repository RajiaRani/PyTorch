{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0d1a496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined1: tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0.]])\n",
      "preact1: tensor([[0.5000, 1.0000, 1.5000, 2.0000]])\n",
      "h1: tensor([[0.4621, 0.7616, 0.9051, 0.9640]])\n",
      "------------------------------------------------------------\n",
      "combined2: tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4621, 0.7616, 0.9051, 0.9640]])\n",
      "preact2: tensor([[0.5093, 1.0186, 1.5279, 2.0372]])\n",
      "h2: tensor([[0.4694, 0.7693, 0.9101, 0.9666]])\n",
      "------------------------------------------------------------\n",
      "combined3: tensor([[0.0000, 2.0000, 0.0000, 0.0000, 0.0000, 0.4694, 0.7693, 0.9101, 0.9666]])\n",
      "preact3: tensor([[0.5115, 1.0231, 1.5346, 2.0461]])\n",
      "h3: tensor([[0.4711, 0.7711, 0.9112, 0.9671]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def linear(x, W, b):\n",
    "    \"\"\"\n",
    "    x: (1, in_features)\n",
    "    W: (out_features, in_features)\n",
    "    b: (out_features,)\n",
    "    returns: (1, out_features)\n",
    "    \"\"\"\n",
    "    return x @ W.T + b\n",
    "\n",
    "def rnn_step_concat(x_t, h_prev, W_i2h, b_i2h):\n",
    "    \"\"\"\n",
    "    x_t:   (1, E)\n",
    "    h_prev:(1, H)\n",
    "    W_i2h: (H, E+H)   (because Linear(E+H -> H))\n",
    "    b_i2h: (H,)\n",
    "    returns:\n",
    "      h_t: (1, H)\n",
    "    \"\"\"\n",
    "    combined = torch.cat([x_t, h_prev], dim=1)         # (1, E+H)\n",
    "    preact = linear(combined, W_i2h, b_i2h)            # (1, H)\n",
    "    h_t = torch.tanh(preact)                           # (1, H)\n",
    "    return h_t, combined, preact\n",
    "\n",
    "# -----------------------------\n",
    "# Example sizes like your note:\n",
    "# E=5, H=4  => combined size = 9\n",
    "# -----------------------------\n",
    "E = 5\n",
    "H = 4\n",
    "\n",
    "# 3 time-step inputs (x1, x2, x3), each (1,5)\n",
    "x1 = torch.tensor([[1., 1., 1., 1., 1.]])\n",
    "x2 = torch.tensor([[2., 0., 0., 0., 0.]])\n",
    "x3 = torch.tensor([[0., 2., 0., 0., 0.]])\n",
    "\n",
    "# initial hidden (1,4)\n",
    "h0 = torch.zeros(1, H)\n",
    "\n",
    "# \"Linear(9->4)\" parameters from scratch:\n",
    "# W_i2h shape (4,9), b_i2h shape (4,)\n",
    "# We'll choose easy numbers so you can compute.\n",
    "W_i2h = torch.tensor([\n",
    "    [0.10,0.10,0.10,0.10,0.10, 0.10,0.10,0.10,0.10],  # neuron 1\n",
    "    [0.20,0.20,0.20,0.20,0.20, 0.20,0.20,0.20,0.20],  # neuron 2\n",
    "    [0.30,0.30,0.30,0.30,0.30, 0.30,0.30,0.30,0.30],  # neuron 3\n",
    "    [0.40,0.40,0.40,0.40,0.40, 0.40,0.40,0.40,0.40],  # neuron 4\n",
    "])\n",
    "b_i2h = torch.tensor([0., 0., 0., 0.])\n",
    "\n",
    "# Run 3 steps\n",
    "h1, c1, p1 = rnn_step_concat(x1, h0, W_i2h, b_i2h)\n",
    "h2, c2, p2 = rnn_step_concat(x2, h1, W_i2h, b_i2h)\n",
    "h3, c3, p3 = rnn_step_concat(x3, h2, W_i2h, b_i2h)\n",
    "\n",
    "print(\"combined1:\", c1)\n",
    "print(\"preact1:\", p1)\n",
    "print(\"h1:\", h1)\n",
    "print(\"------\"*10)\n",
    "\n",
    "print(\"combined2:\", c2)\n",
    "print(\"preact2:\", p2)\n",
    "print(\"h2:\", h2)\n",
    "print(\"------\"*10)\n",
    "\n",
    "print(\"combined3:\", c3)\n",
    "print(\"preact3:\", p3)\n",
    "print(\"h3:\", h3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed61c10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size, output_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_dim+hidden_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_dim)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "    \n",
    "    def forward(self, x_t, h_prev):\n",
    "        combined = torch.cat((x_t, h_prev), dim=1)\n",
    "        h_t = torch.tanh(self.i2h(combined))\n",
    "        y_t = self.h2o(h_t)\n",
    "        return h_t, y_t\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57d6cca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae576def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 1) Tiny Dataset\n",
    "# ----------------------------\n",
    "sentences = [\n",
    "    \"what is the capital of india\",\n",
    "    \"what is the capital of usa\",\n",
    "    \"what is the capital of france\",\n",
    "]\n",
    "\n",
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Build Vocabulary\n",
    "# ----------------------------\n",
    "tokens = []\n",
    "for s in sentences:\n",
    "    tokens += tokenize(s)\n",
    "\n",
    "vocab = sorted(set(tokens))\n",
    "vocab = [\"<unk>\"] + vocab\n",
    "\n",
    "stoi = {w:i for i,w in enumerate(vocab)}\n",
    "itos = {i:w for w,i in stoi.items()}\n",
    "V = len(vocab)\n",
    "\n",
    "def encode(text):\n",
    "    return [stoi.get(w, stoi[\"<unk>\"]) for w in tokenize(text)]\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 3) YOUR VanillaRNN\n",
    "# ----------------------------\n",
    "class VanillaRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input2hidden = nn.Linear(input_size+hidden_size, hidden_size)\n",
    "        self.hidden2output = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "    \n",
    "    def forward(self, x_t, h_prev):\n",
    "        combined = torch.cat((x_t, h_prev), dim=1)\n",
    "        h_t = torch.tanh(self.input2hidden(combined))\n",
    "        y_t = self.hidden2output(h_t)\n",
    "        return h_t, y_t\n",
    "    \n",
    "# ----------------------------\n",
    "# 4) Embedding Layer (separate)\n",
    "# ----------------------------\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "\n",
    "    def forward(self, token_id):\n",
    "        return self.embedding(token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7df6410f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 5) Create Model\n",
    "# ----------------------------\n",
    "torch.manual_seed(123)\n",
    "emb_dim = 8\n",
    "hidden_size = 16\n",
    "embedding = EmbeddingLayer(V, emb_dim)\n",
    "rnn = VanillaRNN(emb_dim, hidden_size, V)\n",
    "params = list(embedding.parameters()) + list(rnn.parameters())\n",
    "optimizer = torch.optim.SGD(params, lr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bdb438cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Loss: 23.0505\n",
      "Epoch 40, Loss: 25.1173\n",
      "Epoch 60, Loss: 25.4774\n",
      "Epoch 80, Loss: 24.9905\n",
      "Epoch 100, Loss: 25.4188\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 6) Training Loop\n",
    "# ----------------------------\n",
    "for epoch in range(1, 101):\n",
    "    total_loss = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        token_ids = encode(sentence)\n",
    "        token_ids = torch.tensor(token_ids, dtype=torch.long)\n",
    "\n",
    "        h = rnn.init_hidden()\n",
    "\n",
    "        loss = 0\n",
    "\n",
    "        # next word prediction\n",
    "        for t in range(len(token_ids) - 1):\n",
    "            x_t = embedding(token_ids[t]).unsqueeze(0)   # (1, emb_dim)\n",
    "            target = token_ids[t+1].unsqueeze(0)         # (1,)\n",
    "\n",
    "            h, logits = rnn(x_t, h)\n",
    "\n",
    "            loss += F.cross_entropy(logits, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa1fa168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing...\n",
      "Input: what is the capital of\n",
      "Prediction: france\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 7) Test Prediction\n",
    "# ----------------------------\n",
    "def predict_next_word(prefix):\n",
    "    rnn.eval()\n",
    "    embedding.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ids = encode(prefix)\n",
    "        ids = torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "        h = rnn.init_hidden()\n",
    "\n",
    "        for t in range(len(ids)):\n",
    "            x_t = embedding(ids[t]).unsqueeze(0)\n",
    "            h, logits = rnn(x_t, h)\n",
    "\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        predicted_id = torch.argmax(probs).item()\n",
    "        return itos[predicted_id]\n",
    "\n",
    "print(\"\\nTesting...\")\n",
    "print(\"Input: what is the capital of\")\n",
    "print(\"Prediction:\", predict_next_word(\"what is the capital of\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
