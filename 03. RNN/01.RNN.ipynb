{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "71e83176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the capital of France?</td>\n",
       "      <td>Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the capital of Germany?</td>\n",
       "      <td>Berlin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who wrote 'To Kill a Mockingbird'?</td>\n",
       "      <td>Harper-Lee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the largest planet in our solar system?</td>\n",
       "      <td>Jupiter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the boiling point of water in Celsius?</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          question      answer\n",
       "0                   What is the capital of France?       Paris\n",
       "1                  What is the capital of Germany?      Berlin\n",
       "2               Who wrote 'To Kill a Mockingbird'?  Harper-Lee\n",
       "3  What is the largest planet in our solar system?     Jupiter\n",
       "4   What is the boiling point of water in Celsius?         100"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"100_Unique_QA_Dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d038a779",
   "metadata": {},
   "source": [
    "# Converting our data set into Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "d9e5ff92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic tokenizer\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('?', '')\n",
    "    text = text.replace(\"'\", \"\")\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "c61ee826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what', 'is', 'the', 'capital', 'of', 'france']"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38cc930",
   "metadata": {},
   "source": [
    "vocab is a dictionary.\n",
    "\n",
    "<UNK> means “unknown word”.\n",
    "\n",
    "Index 0 is reserved for unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "c3796f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vocabulory - staring mein empty hai\n",
    "vocab = {'<UNK>':0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06437ca",
   "metadata": {},
   "source": [
    "Take question words -- Take answer words --- Combine them\n",
    "\n",
    "For every word: if it’s new, assign a new number\n",
    "\n",
    "Example: <UNK> already has 0\n",
    "\n",
    "first new word maybe \"what\" gets 1 ---- next \"is\" gets 2 ------- \"the\" gets 3 …etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "09d13e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(row):\n",
    "    tokenized_question = tokenize(row['question'])\n",
    "    tokenized_answer =tokenize(row['answer'])\n",
    "    # print(tokenized_question, tokenized_answer)\n",
    "\n",
    "    # merge question + answer into one vector\n",
    "    merged_tokens = tokenized_question + tokenized_answer\n",
    "    print(merged_tokens)\n",
    "\n",
    "    for token in merged_tokens:\n",
    "        if token not in vocab:\n",
    "            vocab[token] = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "3b681a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'is', 'the', 'capital', 'of', 'france', 'paris']\n",
      "['what', 'is', 'the', 'capital', 'of', 'germany', 'berlin']\n",
      "['who', 'wrote', 'to', 'kill', 'a', 'mockingbird', 'harper-lee']\n",
      "['what', 'is', 'the', 'largest', 'planet', 'in', 'our', 'solar', 'system', 'jupiter']\n",
      "['what', 'is', 'the', 'boiling', 'point', 'of', 'water', 'in', 'celsius', '100']\n",
      "['who', 'painted', 'the', 'mona', 'lisa', 'leonardo-da-vinci']\n",
      "['what', 'is', 'the', 'square', 'root', 'of', '64', '8']\n",
      "['what', 'is', 'the', 'chemical', 'symbol', 'for', 'gold', 'au']\n",
      "['which', 'year', 'did', 'world', 'war', 'ii', 'end', '1945']\n",
      "['what', 'is', 'the', 'longest', 'river', 'in', 'the', 'world', 'nile']\n",
      "['what', 'is', 'the', 'capital', 'of', 'japan', 'tokyo']\n",
      "['who', 'developed', 'the', 'theory', 'of', 'relativity', 'albert-einstein']\n",
      "['what', 'is', 'the', 'freezing', 'point', 'of', 'water', 'in', 'fahrenheit', '32']\n",
      "['which', 'planet', 'is', 'known', 'as', 'the', 'red', 'planet', 'mars']\n",
      "['who', 'is', 'the', 'author', 'of', '1984', 'george-orwell']\n",
      "['what', 'is', 'the', 'currency', 'of', 'the', 'united', 'kingdom', 'pound']\n",
      "['what', 'is', 'the', 'capital', 'of', 'india', 'delhi']\n",
      "['who', 'discovered', 'gravity', 'newton']\n",
      "['how', 'many', 'continents', 'are', 'there', 'on', 'earth', '7']\n",
      "['which', 'gas', 'do', 'plants', 'use', 'for', 'photosynthesis', 'co2']\n",
      "['what', 'is', 'the', 'smallest', 'prime', 'number', '2']\n",
      "['who', 'invented', 'the', 'telephone', 'alexander-graham-bell']\n",
      "['what', 'is', 'the', 'capital', 'of', 'australia', 'canberra']\n",
      "['which', 'ocean', 'is', 'the', 'largest', 'pacific-ocean']\n",
      "['what', 'is', 'the', 'speed', 'of', 'light', 'in', 'vacuum', '299,792,458m/s']\n",
      "['which', 'language', 'is', 'spoken', 'in', 'brazil', 'portuguese']\n",
      "['who', 'discovered', 'penicillin', 'alexander-fleming']\n",
      "['what', 'is', 'the', 'capital', 'of', 'canada', 'ottawa']\n",
      "['what', 'is', 'the', 'largest', 'mammal', 'on', 'earth', 'whale']\n",
      "['which', 'element', 'has', 'the', 'atomic', 'number', '1', 'hydrogen']\n",
      "['what', 'is', 'the', 'tallest', 'mountain', 'in', 'the', 'world', 'everest']\n",
      "['which', 'city', 'is', 'known', 'as', 'the', 'big', 'apple', 'newyork']\n",
      "['how', 'many', 'planets', 'are', 'in', 'the', 'solar', 'system', '8']\n",
      "['who', 'painted', 'starry', 'night', 'vangogh']\n",
      "['what', 'is', 'the', 'chemical', 'formula', 'of', 'water', 'h2o']\n",
      "['what', 'is', 'the', 'capital', 'of', 'italy', 'rome']\n",
      "['which', 'country', 'is', 'famous', 'for', 'sushi', 'japan']\n",
      "['who', 'was', 'the', 'first', 'person', 'to', 'step', 'on', 'the', 'moon', 'armstrong']\n",
      "['what', 'is', 'the', 'main', 'ingredient', 'in', 'guacamole', 'avocado']\n",
      "['how', 'many', 'sides', 'does', 'a', 'hexagon', 'have', '6']\n",
      "['what', 'is', 'the', 'currency', 'of', 'china', 'yuan']\n",
      "['who', 'wrote', 'pride', 'and', 'prejudice', 'jane-austen']\n",
      "['what', 'is', 'the', 'chemical', 'symbol', 'for', 'iron', 'fe']\n",
      "['what', 'is', 'the', 'hardest', 'natural', 'substance', 'on', 'earth', 'diamond']\n",
      "['which', 'continent', 'is', 'the', 'largest', 'by', 'area', 'asia']\n",
      "['who', 'was', 'the', 'first', 'president', 'of', 'the', 'united', 'states', 'george-washington']\n",
      "['which', 'bird', 'is', 'known', 'for', 'its', 'ability', 'to', 'mimic', 'sounds', 'parrot']\n",
      "['what', 'is', 'the', 'longest-running', 'animated', 'tv', 'show', 'simpsons']\n",
      "['what', 'is', 'the', 'smallest', 'country', 'in', 'the', 'world', 'vaticancity']\n",
      "['which', 'planet', 'has', 'the', 'most', 'moons', 'saturn']\n",
      "['who', 'wrote', 'romeo', 'and', 'juliet', 'shakespeare']\n",
      "['what', 'is', 'the', 'main', 'gas', 'in', 'earths', 'atmosphere', 'nitrogen']\n",
      "['how', 'many', 'bones', 'are', 'in', 'the', 'adult', 'human', 'body', '206']\n",
      "['which', 'metal', 'is', 'a', 'liquid', 'at', 'room', 'temperature', 'mercury']\n",
      "['what', 'is', 'the', 'capital', 'of', 'russia', 'moscow']\n",
      "['who', 'discovered', 'electricity', 'benjamin-franklin']\n",
      "['which', 'is', 'the', 'second-largest', 'country', 'by', 'land', 'area', 'canada']\n",
      "['what', 'is', 'the', 'color', 'of', 'a', 'ripe', 'banana', 'yellow']\n",
      "['which', 'month', 'has', '28', 'days', 'in', 'a', 'common', 'year', 'february']\n",
      "['what', 'is', 'the', 'study', 'of', 'living', 'organisms', 'called', 'biology']\n",
      "['which', 'country', 'is', 'home', 'to', 'the', 'great', 'wall', 'china']\n",
      "['what', 'do', 'bees', 'collect', 'from', 'flowers', 'nectar']\n",
      "['what', 'is', 'the', 'opposite', 'of', 'day', 'night']\n",
      "['what', 'is', 'the', 'capital', 'of', 'south', 'korea', 'seoul']\n",
      "['who', 'invented', 'the', 'light', 'bulb', 'edison']\n",
      "['which', 'gas', 'do', 'humans', 'breathe', 'in', 'for', 'survival', 'oxygen']\n",
      "['what', 'is', 'the', 'square', 'root', 'of', '144', '12']\n",
      "['which', 'country', 'has', 'the', 'pyramids', 'of', 'giza', 'egypt']\n",
      "['which', 'sea', 'creature', 'has', 'eight', 'arms', 'octopus']\n",
      "['which', 'holiday', 'is', 'celebrated', 'on', 'december', '25', 'christmas']\n",
      "['what', 'is', 'the', 'currency', 'of', 'japan', 'yen']\n",
      "['how', 'many', 'legs', 'does', 'a', 'spider', 'have', '8']\n",
      "['which', 'sport', 'uses', 'a', 'net,', 'ball,', 'and', 'hoop', 'basketball']\n",
      "['which', 'country', 'is', 'famous', 'for', 'its', 'kangaroos', 'australia']\n",
      "['who', 'was', 'the', 'first', 'female', 'prime', 'minister', 'of', 'the', 'uk', 'margaretthatcher']\n",
      "['which', 'is', 'the', 'fastest', 'land', 'animal', 'cheetah']\n",
      "['what', 'is', 'the', 'first', 'element', 'on', 'the', 'periodic', 'table', 'hydrogen']\n",
      "['what', 'is', 'the', 'capital', 'of', 'spain', 'madrid']\n",
      "['which', 'planet', 'is', 'the', 'closest', 'to', 'the', 'sun', 'mercury']\n",
      "['who', 'is', 'known', 'as', 'the', 'father', 'of', 'computers', 'charlesbabbage']\n",
      "['what', 'is', 'the', 'capital', 'of', 'mexico', 'mexicocity']\n",
      "['how', 'many', 'colors', 'are', 'in', 'a', 'rainbow', '7']\n",
      "['which', 'musical', 'instrument', 'has', 'black', 'and', 'white', 'keys', 'piano']\n",
      "['who', 'discovered', 'the', 'americas', 'in', '1492', 'christophercolumbus']\n",
      "['which', 'disney', 'character', 'has', 'a', 'long', 'nose', 'and', 'grows', 'it', 'when', 'lying', 'pinocchio']\n",
      "['who', 'directed', 'the', 'movie', 'titanic', 'jamescameron']\n",
      "['which', 'superhero', 'is', 'also', 'known', 'as', 'the', 'dark', 'knight', 'batman']\n",
      "['what', 'is', 'the', 'capital', 'of', 'brazil', 'brasilia']\n",
      "['which', 'fruit', 'is', 'known', 'as', 'the', 'king', 'of', 'fruits', 'mango']\n",
      "['which', 'country', 'is', 'known', 'for', 'the', 'eiffel', 'tower', 'france']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     None\n",
       "1     None\n",
       "2     None\n",
       "3     None\n",
       "4     None\n",
       "      ... \n",
       "85    None\n",
       "86    None\n",
       "87    None\n",
       "88    None\n",
       "89    None\n",
       "Length: 90, dtype: object"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.apply(build_vocab, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "8c77f273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "324"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "b0ce2db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now assign that vocab to index \n",
    "def text_to_indices(text, vocab):\n",
    "    indexed_text = []\n",
    "    for token in tokenize(text):\n",
    "        if token in vocab:\n",
    "            indexed_text.append(vocab[token])\n",
    "        else:\n",
    "            indexed_text.append(vocab['<UNK>'])\n",
    "\n",
    "    return indexed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "3c7ccdd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 0, 0, 2, 0, 0]"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_indices(\"What is campux rajia is my friend\", vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "53af1e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 99, 100]"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_indices(\"'what is the capital of australia canberra\", vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "b3ed9963",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "2a9ef63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QADataset(Dataset):\n",
    "    def __init__(self, df, vocab):\n",
    "        self.df = df\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "      numerical_question  = text_to_indices(self.df.iloc[index]['question'], self.vocab)\n",
    "      numerical_answer  = text_to_indices(self.df.iloc[index]['answer'], self.vocab)\n",
    "       \n",
    "     # converting into tensor\n",
    "      return torch.tensor(numerical_question), torch.tensor( numerical_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "20e4ad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = QADataset(df, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "76a0f672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([1, 2, 3, 4, 5, 6]), tensor([7]))\n",
      "(tensor([1, 2, 3, 4, 5, 8]), tensor([9]))\n",
      "(tensor([10, 11, 12, 13, 14, 15]), tensor([16]))\n",
      "(tensor([ 1,  2,  3, 17, 18, 19, 20, 21, 22]), tensor([23]))\n",
      "(tensor([ 1,  2,  3, 24, 25,  5, 26, 19, 27]), tensor([28]))\n",
      "(tensor([10, 29,  3, 30, 31]), tensor([32]))\n",
      "(tensor([ 1,  2,  3, 33, 34,  5, 35]), tensor([36]))\n",
      "(tensor([ 1,  2,  3, 37, 38, 39, 40]), tensor([41]))\n",
      "(tensor([42, 43, 44, 45, 46, 47, 48]), tensor([49]))\n",
      "(tensor([ 1,  2,  3, 50, 51, 19,  3, 45]), tensor([52]))\n",
      "(tensor([ 1,  2,  3,  4,  5, 53]), tensor([54]))\n",
      "(tensor([10, 55,  3, 56,  5, 57]), tensor([58]))\n",
      "(tensor([ 1,  2,  3, 59, 25,  5, 26, 19, 60]), tensor([61]))\n",
      "(tensor([42, 18,  2, 62, 63,  3, 64, 18]), tensor([65]))\n",
      "(tensor([10,  2,  3, 66,  5, 67]), tensor([68]))\n",
      "(tensor([ 1,  2,  3, 69,  5,  3, 70, 71]), tensor([72]))\n",
      "(tensor([ 1,  2,  3,  4,  5, 73]), tensor([74]))\n",
      "(tensor([10, 75, 76]), tensor([77]))\n",
      "(tensor([78, 79, 80, 81, 82, 83, 84]), tensor([85]))\n",
      "(tensor([42, 86, 87, 88, 89, 39, 90]), tensor([91]))\n",
      "(tensor([ 1,  2,  3, 92, 93, 94]), tensor([95]))\n",
      "(tensor([10, 96,  3, 97]), tensor([98]))\n",
      "(tensor([ 1,  2,  3,  4,  5, 99]), tensor([100]))\n",
      "(tensor([ 42, 101,   2,   3,  17]), tensor([102]))\n",
      "(tensor([  1,   2,   3, 103,   5, 104,  19, 105]), tensor([106]))\n",
      "(tensor([ 42, 107,   2, 108,  19, 109]), tensor([110]))\n",
      "(tensor([ 10,  75, 111]), tensor([112]))\n",
      "(tensor([  1,   2,   3,   4,   5, 113]), tensor([114]))\n",
      "(tensor([  1,   2,   3,  17, 115,  83,  84]), tensor([116]))\n",
      "(tensor([ 42, 117, 118,   3, 119,  94, 120]), tensor([121]))\n",
      "(tensor([  1,   2,   3, 122, 123,  19,   3,  45]), tensor([124]))\n",
      "(tensor([ 42, 125,   2,  62,  63,   3, 126, 127]), tensor([128]))\n",
      "(tensor([ 78,  79, 129,  81,  19,   3,  21,  22]), tensor([36]))\n",
      "(tensor([ 10,  29, 130, 131]), tensor([132]))\n",
      "(tensor([  1,   2,   3,  37, 133,   5,  26]), tensor([134]))\n",
      "(tensor([  1,   2,   3,   4,   5, 135]), tensor([136]))\n",
      "(tensor([ 42, 137,   2, 138,  39, 139]), tensor([53]))\n",
      "(tensor([ 10, 140,   3, 141, 142,  12, 143,  83,   3, 144]), tensor([145]))\n",
      "(tensor([  1,   2,   3, 146, 147,  19, 148]), tensor([149]))\n",
      "(tensor([ 78,  79, 150, 151,  14, 152, 153]), tensor([154]))\n",
      "(tensor([  1,   2,   3,  69,   5, 155]), tensor([156]))\n",
      "(tensor([ 10,  11, 157, 158, 159]), tensor([160]))\n",
      "(tensor([  1,   2,   3,  37,  38,  39, 161]), tensor([162]))\n",
      "(tensor([  1,   2,   3, 163, 164, 165,  83,  84]), tensor([166]))\n",
      "(tensor([ 42, 167,   2,   3,  17, 168, 169]), tensor([170]))\n",
      "(tensor([ 10, 140,   3, 141, 171,   5,   3,  70, 172]), tensor([173]))\n",
      "(tensor([ 42, 174,   2,  62,  39, 175, 176,  12, 177, 178]), tensor([179]))\n",
      "(tensor([  1,   2,   3, 180, 181, 182, 183]), tensor([184]))\n",
      "(tensor([  1,   2,   3,  92, 137,  19,   3,  45]), tensor([185]))\n",
      "(tensor([ 42,  18, 118,   3, 186, 187]), tensor([188]))\n",
      "(tensor([ 10,  11, 189, 158, 190]), tensor([191]))\n",
      "(tensor([  1,   2,   3, 146,  86,  19, 192, 193]), tensor([194]))\n",
      "(tensor([ 78,  79, 195,  81,  19,   3, 196, 197, 198]), tensor([199]))\n",
      "(tensor([ 42, 200,   2,  14, 201, 202, 203, 204]), tensor([205]))\n",
      "(tensor([  1,   2,   3,   4,   5, 206]), tensor([207]))\n",
      "(tensor([ 10,  75, 208]), tensor([209]))\n",
      "(tensor([ 42,   2,   3, 210, 137, 168, 211, 169]), tensor([113]))\n",
      "(tensor([  1,   2,   3, 212,   5,  14, 213, 214]), tensor([215]))\n",
      "(tensor([ 42, 216, 118, 217, 218,  19,  14, 219,  43]), tensor([220]))\n",
      "(tensor([  1,   2,   3, 221,   5, 222, 223, 224]), tensor([225]))\n",
      "(tensor([ 42, 137,   2, 226,  12,   3, 227, 228]), tensor([155]))\n",
      "(tensor([  1,  87, 229, 230, 231, 232]), tensor([233]))\n",
      "(tensor([  1,   2,   3, 234,   5, 235]), tensor([131]))\n",
      "(tensor([  1,   2,   3,   4,   5, 236, 237]), tensor([238]))\n",
      "(tensor([ 10,  96,   3, 104, 239]), tensor([240]))\n",
      "(tensor([ 42,  86,  87, 241, 242,  19,  39, 243]), tensor([244]))\n",
      "(tensor([  1,   2,   3,  33,  34,   5, 245]), tensor([246]))\n",
      "(tensor([ 42, 137, 118,   3, 247,   5, 248]), tensor([249]))\n",
      "(tensor([ 42, 250, 251, 118, 252, 253]), tensor([254]))\n",
      "(tensor([ 42, 255,   2, 256,  83, 257, 258]), tensor([259]))\n",
      "(tensor([ 1,  2,  3, 69,  5, 53]), tensor([260]))\n",
      "(tensor([ 78,  79, 261, 151,  14, 262, 153]), tensor([36]))\n",
      "(tensor([ 42, 263, 264,  14, 265, 266, 158, 267]), tensor([268]))\n",
      "(tensor([ 42, 137,   2, 138,  39, 175, 269]), tensor([99]))\n",
      "(tensor([ 10, 140,   3, 141, 270,  93, 271,   5,   3, 272]), tensor([273]))\n",
      "(tensor([ 42,   2,   3, 274, 211, 275]), tensor([276]))\n",
      "(tensor([  1,   2,   3, 141, 117,  83,   3, 277, 278]), tensor([121]))\n",
      "(tensor([  1,   2,   3,   4,   5, 279]), tensor([280]))\n",
      "(tensor([ 42,  18,   2,   3, 281,  12,   3, 282]), tensor([205]))\n",
      "(tensor([ 10,   2,  62,  63,   3, 283,   5, 284]), tensor([285]))\n",
      "(tensor([  1,   2,   3,   4,   5, 286]), tensor([287]))\n",
      "(tensor([ 78,  79, 288,  81,  19,  14, 289]), tensor([85]))\n",
      "(tensor([ 42, 290, 291, 118, 292, 158, 293, 294]), tensor([295]))\n",
      "(tensor([ 10,  75,   3, 296,  19, 297]), tensor([298]))\n",
      "(tensor([ 42, 299, 300, 118,  14, 301, 302, 158, 303, 304, 305, 306]), tensor([307]))\n",
      "(tensor([ 10, 308,   3, 309, 310]), tensor([311]))\n",
      "(tensor([ 42, 312,   2, 313,  62,  63,   3, 314, 315]), tensor([316]))\n",
      "(tensor([  1,   2,   3,   4,   5, 109]), tensor([317]))\n",
      "(tensor([ 42, 318,   2,  62,  63,   3, 319,   5, 320]), tensor([321]))\n",
      "(tensor([ 42, 137,   2,  62,  39,   3, 322, 323]), tensor([6]))\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(dataset)):\n",
    "    print(dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "9bdc6ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "e13e56b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 42, 117, 118,   3, 119,  94, 120]]) tensor([121])\n",
      "tensor([[ 42, 263, 264,  14, 265, 266, 158, 267]]) tensor([268])\n",
      "tensor([[ 78,  79, 150, 151,  14, 152, 153]]) tensor([154])\n",
      "tensor([[ 42, 255,   2, 256,  83, 257, 258]]) tensor([259])\n",
      "tensor([[1, 2, 3, 4, 5, 6]]) tensor([7])\n",
      "tensor([[  1,   2,   3, 234,   5, 235]]) tensor([131])\n",
      "tensor([[78, 79, 80, 81, 82, 83, 84]]) tensor([85])\n",
      "tensor([[ 10,  96,   3, 104, 239]]) tensor([240])\n",
      "tensor([[  1,   2,   3, 212,   5,  14, 213, 214]]) tensor([215])\n",
      "tensor([[ 1,  2,  3,  4,  5, 53]]) tensor([54])\n",
      "tensor([[ 10, 308,   3, 309, 310]]) tensor([311])\n",
      "tensor([[  1,   2,   3, 163, 164, 165,  83,  84]]) tensor([166])\n",
      "tensor([[ 42,   2,   3, 210, 137, 168, 211, 169]]) tensor([113])\n",
      "tensor([[ 78,  79, 261, 151,  14, 262, 153]]) tensor([36])\n",
      "tensor([[  1,   2,   3,  33,  34,   5, 245]]) tensor([246])\n",
      "tensor([[  1,   2,   3,   4,   5, 109]]) tensor([317])\n",
      "tensor([[ 10, 140,   3, 141, 171,   5,   3,  70, 172]]) tensor([173])\n",
      "tensor([[ 42, 137, 118,   3, 247,   5, 248]]) tensor([249])\n",
      "tensor([[10, 96,  3, 97]]) tensor([98])\n",
      "tensor([[  1,   2,   3, 103,   5, 104,  19, 105]]) tensor([106])\n",
      "tensor([[ 78,  79, 129,  81,  19,   3,  21,  22]]) tensor([36])\n",
      "tensor([[ 1,  2,  3, 33, 34,  5, 35]]) tensor([36])\n",
      "tensor([[ 10,  29, 130, 131]]) tensor([132])\n",
      "tensor([[  1,   2,   3, 146,  86,  19, 192, 193]]) tensor([194])\n",
      "tensor([[ 78,  79, 195,  81,  19,   3, 196, 197, 198]]) tensor([199])\n",
      "tensor([[ 78,  79, 288,  81,  19,  14, 289]]) tensor([85])\n",
      "tensor([[ 1,  2,  3,  4,  5, 73]]) tensor([74])\n",
      "tensor([[ 10,  11, 157, 158, 159]]) tensor([160])\n",
      "tensor([[ 42, 107,   2, 108,  19, 109]]) tensor([110])\n",
      "tensor([[  1,   2,   3,   4,   5, 135]]) tensor([136])\n",
      "tensor([[ 42, 290, 291, 118, 292, 158, 293, 294]]) tensor([295])\n",
      "tensor([[ 10,  11, 189, 158, 190]]) tensor([191])\n",
      "tensor([[10, 11, 12, 13, 14, 15]]) tensor([16])\n",
      "tensor([[ 42,  86,  87, 241, 242,  19,  39, 243]]) tensor([244])\n",
      "tensor([[  1,   2,   3,   4,   5, 286]]) tensor([287])\n",
      "tensor([[ 42, 101,   2,   3,  17]]) tensor([102])\n",
      "tensor([[1, 2, 3, 4, 5, 8]]) tensor([9])\n",
      "tensor([[ 1,  2,  3,  4,  5, 99]]) tensor([100])\n",
      "tensor([[ 42,   2,   3, 274, 211, 275]]) tensor([276])\n",
      "tensor([[  1,  87, 229, 230, 231, 232]]) tensor([233])\n",
      "tensor([[ 1,  2,  3, 24, 25,  5, 26, 19, 27]]) tensor([28])\n",
      "tensor([[42, 43, 44, 45, 46, 47, 48]]) tensor([49])\n",
      "tensor([[42, 86, 87, 88, 89, 39, 90]]) tensor([91])\n",
      "tensor([[  1,   2,   3, 180, 181, 182, 183]]) tensor([184])\n",
      "tensor([[  1,   2,   3,  92, 137,  19,   3,  45]]) tensor([185])\n",
      "tensor([[  1,   2,   3,   4,   5, 206]]) tensor([207])\n",
      "tensor([[ 42,  18,   2,   3, 281,  12,   3, 282]]) tensor([205])\n",
      "tensor([[ 42, 125,   2,  62,  63,   3, 126, 127]]) tensor([128])\n",
      "tensor([[  1,   2,   3,  69,   5, 155]]) tensor([156])\n",
      "tensor([[ 42, 167,   2,   3,  17, 168, 169]]) tensor([170])\n",
      "tensor([[ 10, 140,   3, 141, 270,  93, 271,   5,   3, 272]]) tensor([273])\n",
      "tensor([[  1,   2,   3, 221,   5, 222, 223, 224]]) tensor([225])\n",
      "tensor([[ 1,  2,  3, 69,  5, 53]]) tensor([260])\n",
      "tensor([[10, 75, 76]]) tensor([77])\n",
      "tensor([[ 10,  75,   3, 296,  19, 297]]) tensor([298])\n",
      "tensor([[ 42, 137,   2, 226,  12,   3, 227, 228]]) tensor([155])\n",
      "tensor([[ 42, 137,   2,  62,  39,   3, 322, 323]]) tensor([6])\n",
      "tensor([[ 42, 174,   2,  62,  39, 175, 176,  12, 177, 178]]) tensor([179])\n",
      "tensor([[  1,   2,   3,  17, 115,  83,  84]]) tensor([116])\n",
      "tensor([[ 10,  75, 111]]) tensor([112])\n",
      "tensor([[ 1,  2,  3, 37, 38, 39, 40]]) tensor([41])\n",
      "tensor([[ 42, 137,   2, 138,  39, 139]]) tensor([53])\n",
      "tensor([[ 1,  2,  3, 50, 51, 19,  3, 45]]) tensor([52])\n",
      "tensor([[10, 29,  3, 30, 31]]) tensor([32])\n",
      "tensor([[ 42, 250, 251, 118, 252, 253]]) tensor([254])\n",
      "tensor([[ 10, 140,   3, 141, 142,  12, 143,  83,   3, 144]]) tensor([145])\n",
      "tensor([[  1,   2,   3, 141, 117,  83,   3, 277, 278]]) tensor([121])\n",
      "tensor([[  1,   2,   3, 146, 147,  19, 148]]) tensor([149])\n",
      "tensor([[ 42, 312,   2, 313,  62,  63,   3, 314, 315]]) tensor([316])\n",
      "tensor([[  1,   2,   3,   4,   5, 113]]) tensor([114])\n",
      "tensor([[ 42, 216, 118, 217, 218,  19,  14, 219,  43]]) tensor([220])\n",
      "tensor([[ 10,  75, 208]]) tensor([209])\n",
      "tensor([[  1,   2,   3,  37,  38,  39, 161]]) tensor([162])\n",
      "tensor([[  1,   2,   3,  37, 133,   5,  26]]) tensor([134])\n",
      "tensor([[42, 18,  2, 62, 63,  3, 64, 18]]) tensor([65])\n",
      "tensor([[ 42, 299, 300, 118,  14, 301, 302, 158, 303, 304, 305, 306]]) tensor([307])\n",
      "tensor([[  1,   2,   3, 122, 123,  19,   3,  45]]) tensor([124])\n",
      "tensor([[ 42,  18, 118,   3, 186, 187]]) tensor([188])\n",
      "tensor([[10,  2,  3, 66,  5, 67]]) tensor([68])\n",
      "tensor([[ 10,   2,  62,  63,   3, 283,   5, 284]]) tensor([285])\n",
      "tensor([[ 42, 137,   2, 138,  39, 175, 269]]) tensor([99])\n",
      "tensor([[ 1,  2,  3, 92, 93, 94]]) tensor([95])\n",
      "tensor([[ 1,  2,  3, 17, 18, 19, 20, 21, 22]]) tensor([23])\n",
      "tensor([[ 1,  2,  3, 59, 25,  5, 26, 19, 60]]) tensor([61])\n",
      "tensor([[ 42, 318,   2,  62,  63,   3, 319,   5, 320]]) tensor([321])\n",
      "tensor([[ 42, 200,   2,  14, 201, 202, 203, 204]]) tensor([205])\n",
      "tensor([[10, 55,  3, 56,  5, 57]]) tensor([58])\n",
      "tensor([[  1,   2,   3,   4,   5, 279]]) tensor([280])\n",
      "tensor([[  1,   2,   3,   4,   5, 236, 237]]) tensor([238])\n",
      "tensor([[ 1,  2,  3, 69,  5,  3, 70, 71]]) tensor([72])\n"
     ]
    }
   ],
   "source": [
    "for question, answer in dataloader:\n",
    "    # print(question[0], answer[0])\n",
    "\n",
    "    # Humare dataloasder mein jp answer hai wo 2d vector hai - but it should be in 1d vector. -- print(question, answer)\n",
    "    print(question, answer[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d9a418",
   "metadata": {},
   "source": [
    "# RNN - Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "ea4e7a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "27557b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "\n",
    "       super().__init__()\n",
    "       self.embedding = nn.Embedding(vocab_size, embedding_dim=50)\n",
    "       self.rnn = nn.RNN(50, 64, batch_first=True)\n",
    "       self.fc = nn.Linear(64, vocab_size)\n",
    "\n",
    "    def forward(self,question):\n",
    "        embedded_question = self.embedding(question)\n",
    "        hidden, final = self.rnn(embedded_question)\n",
    "        output = self.fc(final.squeeze(0))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "e51d40e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = nn.Embedding(324, embedding_dim=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "33baed5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = x(dataset[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "afde009e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4620e+00,  1.9749e-01,  1.5101e+00,  6.3097e-02,  1.3513e+00,\n",
       "          2.3065e+00, -1.1935e+00, -6.5576e-01, -3.6879e-01, -3.4551e-02,\n",
       "         -2.0489e-01, -4.8504e-01, -1.3403e+00,  1.5944e+00,  5.2382e-01,\n",
       "          5.1665e-01,  1.4921e+00,  1.4194e+00,  1.0114e+00, -9.3792e-01,\n",
       "         -2.2628e+00,  1.1561e+00, -5.9504e-01,  1.7646e+00, -1.5969e+00,\n",
       "         -6.1401e-01,  1.1085e+00,  2.8678e-01, -1.0663e+00,  3.9252e-01,\n",
       "         -1.8657e+00, -1.2627e+00,  1.3352e+00,  1.5979e+00, -5.4800e-01,\n",
       "         -4.1947e-01, -1.8943e+00,  8.1749e-01, -1.7506e-01,  4.9213e-01,\n",
       "         -1.0683e+00,  1.3749e+00, -9.3097e-01,  1.4183e-01, -6.1944e-01,\n",
       "         -1.0037e+00,  6.0973e-01,  7.5424e-01, -1.0513e+00, -7.2630e-01],\n",
       "        [-1.7000e+00, -7.0641e-01, -1.2578e+00, -4.8259e-01,  1.0410e+00,\n",
       "         -8.2163e-02, -2.2141e-01, -4.2804e-01,  4.1868e-01,  4.2446e-01,\n",
       "         -5.6893e-01,  6.0598e-01,  4.1341e-01, -9.4245e-01, -7.2824e-01,\n",
       "         -9.2631e-01, -2.1766e-01, -1.3855e+00,  1.3027e+00, -1.6479e+00,\n",
       "          1.0893e+00, -2.4459e-02, -1.5609e+00, -5.1773e-01, -1.0381e+00,\n",
       "         -1.4320e-01,  1.3379e-01, -1.2750e+00, -1.5264e+00, -3.1242e-01,\n",
       "         -2.7282e+00, -5.5179e-01,  4.3536e-01, -4.3813e-01, -1.9684e-02,\n",
       "          2.1644e-01, -2.3677e-01,  1.7803e+00,  1.3094e+00,  1.8595e+00,\n",
       "         -1.1798e+00, -1.0116e+00, -6.1451e-01, -1.2167e+00, -2.0927e-01,\n",
       "          1.1423e+00,  2.5310e-01,  2.7111e-01,  3.5373e-01,  9.0243e-01],\n",
       "        [ 4.5069e-01, -3.4674e-01,  3.4308e-01, -2.1472e-01,  1.3114e-01,\n",
       "         -1.4785e-01,  1.3152e+00, -6.7029e-01,  8.5463e-01,  6.4731e-01,\n",
       "         -9.6929e-01, -1.3557e+00,  2.4569e-01,  1.3145e+00,  1.5552e-01,\n",
       "          3.6823e-01,  9.7780e-02,  6.7576e-02, -1.5961e-01,  1.2270e-01,\n",
       "          1.2470e+00, -5.7510e-01,  3.5345e-01,  5.1625e-01,  1.7811e+00,\n",
       "          6.4206e-01,  1.7305e-01,  1.5399e+00,  1.8314e+00, -2.9676e-01,\n",
       "         -1.5681e+00, -1.0306e+00, -8.0958e-01, -1.0349e+00, -4.2327e-01,\n",
       "          3.2469e-01, -6.3778e-01, -1.5071e-01,  4.9142e-01,  1.9723e-03,\n",
       "          1.5598e+00, -8.2475e-01, -1.8946e-01,  8.6980e-01,  1.5876e+00,\n",
       "          1.1820e+00, -2.5342e-01, -3.6755e-01,  9.4442e-01, -8.9261e-01],\n",
       "        [ 2.0688e-01,  3.3756e-01, -2.0673e-01, -4.7253e-01, -4.0214e-01,\n",
       "         -7.9057e-01, -1.6783e+00, -7.9788e-02,  1.2807e+00,  7.1335e-01,\n",
       "         -1.9414e+00, -9.9082e-02,  1.7217e-01,  4.9224e-01, -8.3440e-02,\n",
       "         -4.4613e-01, -1.7859e+00,  4.8869e-01,  1.2337e+00, -1.3654e-01,\n",
       "          7.8873e-01, -8.2365e-01,  1.0362e+00,  7.4222e-01,  2.3597e+00,\n",
       "          1.1056e+00,  3.1971e-01, -1.4064e-01,  1.4947e+00, -7.2125e-01,\n",
       "          3.8720e-02,  5.2441e-01,  5.4811e-01,  2.0748e-01, -1.2563e+00,\n",
       "          3.3817e-01, -7.0315e-01,  4.2367e-01,  7.4403e-01, -8.7145e-01,\n",
       "         -2.5414e-01, -3.8706e-01,  6.8203e-01,  8.0775e-01,  1.0512e+00,\n",
       "         -1.5244e+00, -1.3167e+00,  3.4357e-01,  4.1486e-01,  1.4157e+00],\n",
       "        [ 4.2462e-02,  3.6551e-01,  4.4791e-01,  2.1879e+00,  1.1318e+00,\n",
       "         -9.3985e-01, -2.3286e-01,  5.5139e-01,  1.1508e+00, -1.1402e+00,\n",
       "         -3.8111e-01, -2.9372e-01, -2.7504e-01,  6.3508e-01, -3.3697e-01,\n",
       "          1.0302e+00,  9.5916e-01,  2.5043e+00, -7.1660e-01,  2.0180e+00,\n",
       "         -6.1030e-01,  4.9719e-01, -5.1165e-01, -1.3114e+00,  3.0972e-03,\n",
       "         -4.6685e-02, -7.9869e-01, -1.7470e+00,  2.4449e-01, -6.1630e-01,\n",
       "          1.2309e+00,  1.9039e+00, -1.0553e+00, -2.7704e-01,  8.4536e-01,\n",
       "          6.0520e-01, -7.1297e-01, -1.2687e+00, -3.5572e-01, -1.4976e-01,\n",
       "         -5.8789e-01, -1.5226e+00,  7.9632e-01,  7.5164e-01,  8.2318e-01,\n",
       "         -5.8703e-01, -5.5527e-01,  4.2039e-01, -1.5184e-01,  1.7257e+00],\n",
       "        [ 6.4581e-01, -2.2637e-02,  8.7098e-01,  1.5984e-01, -8.8653e-01,\n",
       "          1.5657e+00, -2.2094e+00,  4.6168e-01, -1.7114e+00, -2.1494e-01,\n",
       "          1.2135e+00, -1.7882e+00,  8.5052e-01,  3.7823e-01,  4.9764e-01,\n",
       "          7.1161e-02, -2.6878e-01, -6.4454e-02,  9.4582e-02, -1.7134e+00,\n",
       "         -2.6650e-01,  6.9724e-01, -1.0499e+00,  3.2236e-01, -1.9423e+00,\n",
       "         -8.1924e-01,  1.0477e+00,  1.0579e+00,  6.7187e-01, -2.3938e-01,\n",
       "         -5.5912e-01,  1.0243e+00,  2.1390e-01,  9.0078e-01,  1.7007e-01,\n",
       "          1.6246e-01,  6.2381e-01,  4.9949e-01, -7.0318e-01,  1.0818e+00,\n",
       "          5.7596e-01, -1.0010e+00,  3.5170e-01, -2.5455e-01,  8.5213e-01,\n",
       "          9.4198e-01, -9.2209e-01, -1.4486e-01, -6.5932e-01, -4.2198e-01]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "93536c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = nn.RNN(50, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "6fd0b08d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.6748, -0.0978,  0.7745, -0.7282, -0.2032, -0.4205,  0.6376,  0.4010,\n",
       "           0.7763,  0.0304, -0.0314, -0.5680,  0.1892,  0.6928,  0.7287,  0.4384,\n",
       "          -0.1925,  0.4610,  0.1900, -0.1199,  0.7262,  0.8446, -0.1629, -0.2603,\n",
       "           0.1100, -0.0192, -0.4366,  0.8797,  0.7424,  0.6088, -0.5715,  0.0332,\n",
       "           0.6566,  0.1976, -0.1022,  0.7170,  0.3550,  0.2337, -0.1382, -0.2252,\n",
       "           0.4143, -0.5490,  0.3431, -0.6302, -0.4066, -0.3680, -0.3326, -0.1669,\n",
       "           0.3796, -0.4306,  0.4659,  0.4090, -0.1525,  0.2867,  0.6836,  0.0056,\n",
       "           0.0965,  0.0213,  0.1343, -0.0308,  0.6674,  0.2255, -0.0410, -0.6681],\n",
       "         [ 0.2422, -0.0967, -0.6369,  0.3543, -0.5496,  0.0021, -0.1403,  0.2546,\n",
       "           0.0281, -0.1655,  0.4442, -0.3394,  0.2801,  0.4661, -0.0466, -0.3867,\n",
       "          -0.7884, -0.2370, -0.3057, -0.1617, -0.5798,  0.2397,  0.3771,  0.1724,\n",
       "           0.4158,  0.5562,  0.1317, -0.2974,  0.7072,  0.0274, -0.0723, -0.4747,\n",
       "          -0.1000, -0.2608, -0.0619,  0.4498, -0.5679, -0.0061, -0.0551, -0.2718,\n",
       "          -0.2468, -0.1273,  0.5586,  0.1556, -0.8945, -0.5058,  0.5734, -0.3694,\n",
       "           0.4648, -0.3485, -0.7039,  0.9279,  0.5301,  0.3799,  0.2394,  0.0201,\n",
       "          -0.3225, -0.6252, -0.2920, -0.0224, -0.3746,  0.3608,  0.3831, -0.2806],\n",
       "         [-0.1720,  0.0919, -0.5235, -0.2752,  0.5520,  0.1651,  0.6787, -0.4513,\n",
       "          -0.0203, -0.0532, -0.3487,  0.0011,  0.7129,  0.0418, -0.0972,  0.2631,\n",
       "          -0.0430, -0.3212,  0.8077,  0.0822,  0.3928, -0.6390,  0.4454, -0.1064,\n",
       "          -0.5106, -0.2141, -0.0838,  0.1298, -0.3486, -0.6419,  0.5102,  0.5776,\n",
       "          -0.4265,  0.1768, -0.7179, -0.5210, -0.0971, -0.0185,  0.2083, -0.3853,\n",
       "           0.0489,  0.5888, -0.3354,  0.3713, -0.1857,  0.2840,  0.4014, -0.0453,\n",
       "          -0.2270, -0.2787,  0.6628,  0.2648, -0.7145,  0.1459, -0.3015,  0.1767,\n",
       "           0.6193,  0.0252, -0.5537,  0.1288,  0.5159, -0.5075, -0.1673, -0.4238],\n",
       "         [-0.3461,  0.0185,  0.2004, -0.6055,  0.8174,  0.4615, -0.1451, -0.2367,\n",
       "           0.3285,  0.3198,  0.5488,  0.2540,  0.8068,  0.8695,  0.3767, -0.4924,\n",
       "          -0.1110, -0.5406,  0.8223, -0.4427, -0.7414, -0.1327, -0.4884,  0.4920,\n",
       "          -0.8824, -0.6901, -0.3475, -0.4756,  0.1675, -0.7848,  0.1671,  0.6651,\n",
       "           0.1929,  0.3193,  0.4278, -0.1252,  0.0743,  0.0136,  0.1846,  0.3959,\n",
       "          -0.3971, -0.1531, -0.6153,  0.1023, -0.1163,  0.7182, -0.2515,  0.2639,\n",
       "           0.3018,  0.4060,  0.3953,  0.0682, -0.8092,  0.6887, -0.2387, -0.1466,\n",
       "          -0.2972,  0.6904, -0.3891, -0.1087,  0.7739, -0.6166,  0.3932, -0.4312],\n",
       "         [ 0.3702, -0.5418,  0.3880, -0.2256,  0.3957, -0.8538, -0.3386,  0.6789,\n",
       "          -0.0483,  0.3811,  0.1430, -0.6609, -0.2541, -0.0540,  0.2984, -0.1853,\n",
       "          -0.4510,  0.2673, -0.6525, -0.4279, -0.3632, -0.6124, -0.8310,  0.1573,\n",
       "          -0.6857, -0.1425,  0.5590,  0.1151, -0.3132,  0.0954,  0.9307, -0.6052,\n",
       "           0.3297,  0.8898,  0.6428, -0.0972,  0.8224, -0.1201, -0.6154, -0.4132,\n",
       "           0.6705, -0.9297, -0.4815,  0.4921, -0.5653,  0.6816,  0.4770, -0.1554,\n",
       "           0.7278,  0.6474, -0.4003, -0.1763, -0.0292, -0.8995,  0.0571, -0.1995,\n",
       "          -0.4741,  0.7075,  0.3224, -0.3038,  0.4150, -0.8951,  0.4711,  0.5902],\n",
       "         [-0.2122, -0.1994,  0.3306,  0.7407, -0.7351,  0.4042,  0.4099,  0.6512,\n",
       "          -0.0862, -0.0921, -0.6115, -0.6434,  0.1286,  0.1060,  0.9244,  0.8148,\n",
       "           0.2900,  0.1444, -0.9218, -0.1627,  0.5273,  0.2797, -0.5711,  0.4984,\n",
       "           0.5840,  0.4226,  0.3319,  0.1992,  0.7654,  0.5275,  0.3225, -0.8003,\n",
       "           0.8399,  0.1202,  0.4938, -0.0519, -0.3466,  0.4368, -0.4199, -0.1899,\n",
       "          -0.0234, -0.3237,  0.6611, -0.3482,  0.6832, -0.6890, -0.0600,  0.3602,\n",
       "           0.6386,  0.3358, -0.4599,  0.4972, -0.4950,  0.4658, -0.0917,  0.4430,\n",
       "          -0.0330, -0.8649,  0.6754,  0.4870, -0.0957, -0.0565,  0.3194, -0.8701]],\n",
       "        grad_fn=<SqueezeBackward1>),\n",
       " tensor([[-0.2122, -0.1994,  0.3306,  0.7407, -0.7351,  0.4042,  0.4099,  0.6512,\n",
       "          -0.0862, -0.0921, -0.6115, -0.6434,  0.1286,  0.1060,  0.9244,  0.8148,\n",
       "           0.2900,  0.1444, -0.9218, -0.1627,  0.5273,  0.2797, -0.5711,  0.4984,\n",
       "           0.5840,  0.4226,  0.3319,  0.1992,  0.7654,  0.5275,  0.3225, -0.8003,\n",
       "           0.8399,  0.1202,  0.4938, -0.0519, -0.3466,  0.4368, -0.4199, -0.1899,\n",
       "          -0.0234, -0.3237,  0.6611, -0.3482,  0.6832, -0.6890, -0.0600,  0.3602,\n",
       "           0.6386,  0.3358, -0.4599,  0.4972, -0.4950,  0.4658, -0.0917,  0.4430,\n",
       "          -0.0330, -0.8649,  0.6754,  0.4870, -0.0957, -0.0565,  0.3194, -0.8701]],\n",
       "        grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "70b37a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = y(a)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "1b8206d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = nn.Linear(64, 324)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "e42bd992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1470, -0.3861,  0.1322,  ...,  0.5383, -0.3491, -0.0231],\n",
       "        [-0.2705,  0.1819,  0.1271,  ...,  0.1125,  0.0487, -0.2165],\n",
       "        [-0.5856,  0.1106,  0.0258,  ..., -0.2900, -0.1779,  0.5624],\n",
       "        [-0.1366, -0.4581,  0.1893,  ...,  0.2446, -0.3306,  0.1017],\n",
       "        [ 0.5694, -0.3378,  0.0104,  ...,  0.0870,  0.4658,  0.2102],\n",
       "        [-0.3030, -0.5053, -0.1261,  ...,  0.2058,  0.1786, -0.0897]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "40f5fdcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 324])"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z(b).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "10b9c76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "epochs = 60\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "55433194",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleRNN(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "3bca79da",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cbb0fb",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "25884a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 60, Loss: 0.542569\n"
     ]
    }
   ],
   "source": [
    "for epoch in range (epochs):\n",
    "  \n",
    "  total_loss = 0\n",
    "\n",
    "  for question , answer in dataloader:\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward pss\n",
    "    output = model(question)\n",
    "    # print(output.shape)\n",
    "    # loss => output shape(1, 324) -- anser(1)\n",
    "    loss = criterion(output, answer[0])\n",
    "\n",
    "    # Gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # Update gradiednt\n",
    "    optimizer.step()\n",
    "\n",
    "    total_loss = total_loss + loss.item()\n",
    "\n",
    "print(f\"Epoch : {epoch+1}, Loss: {total_loss:4f}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "3b9ff9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, question, threshold=0.5):\n",
    "\n",
    "    # convert question to numbers\n",
    "    numerical_question = text_to_indices(question, vocab)\n",
    "\n",
    "    # convert to tensor\n",
    "    question_tensor = torch.tensor(numerical_question).unsqueeze(0)\n",
    "\n",
    "    # send to model\n",
    "    output = model(question_tensor)\n",
    "\n",
    "    #convert logits  to probability\n",
    "    prob = torch.nn.functional.softmax(output, dim=1)\n",
    "    #print(prob)\n",
    "\n",
    "    #print(question_tensor.shape)\n",
    "    \n",
    "\n",
    "    # find the index of max vocab\n",
    "    value , index = torch.max(prob, dim=1)\n",
    "    if value < threshold:\n",
    "        print(\"I do not know\")\n",
    "    print(list(vocab.keys())[index])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "84115027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "berlin\n"
     ]
    }
   ],
   "source": [
    "predict(model, \"What is the capital of Germany \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "7164abea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 324])"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
