{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b6bf07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5c0119a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = [\n",
    "    (\"capital of usa\", \"washington\"),\n",
    "    (\"capital of india\", \"delhi\"),\n",
    "    (\"capital of france\", \"paris\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2544df19",
   "metadata": {},
   "source": [
    "# Tokenize and Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae80e254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return text.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f6b7ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['capital', 'of', 'usa', 'washington', 'capital', 'of', 'india', 'delhi', 'capital', 'of', 'france', 'paris']\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# 2) Build Vocabulary\n",
    "# -----------------------\n",
    "all_tokens = []\n",
    "for q, a in DATA:\n",
    "    all_tokens += tokenize(q)\n",
    "    all_tokens += tokenize(a)\n",
    "print(all_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f527ce5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sort the tokens ===  ['capital', 'delhi', 'france', 'india', 'of', 'paris', 'usa', 'washington']\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(all_tokens))\n",
    "print(\"Sort the tokens === \", vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "424507e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String to Index ==  {'capital': 0, 'delhi': 1, 'france': 2, 'india': 3, 'of': 4, 'paris': 5, 'usa': 6, 'washington': 7}\n",
      "Index to String == {0: 'capital', 1: 'delhi', 2: 'france', 3: 'india', 4: 'of', 5: 'paris', 6: 'usa', 7: 'washington'}\n"
     ]
    }
   ],
   "source": [
    "stoi = {w:i for i,w in enumerate(vocab)}\n",
    "print(\"String to Index == \", stoi)\n",
    "itos = {i:w for w,i in stoi.items()}\n",
    "print(\"Index to String ==\", itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f803452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(tokens):\n",
    "    return [stoi[t] for t in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb7c546",
   "metadata": {},
   "source": [
    "# Build RNN from Scratch (NO batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "846ac70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=4, hidden_size=4):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Embedding matrix (V, E)\n",
    "        self.E = nn.Parameter(torch.empty(vocab_size, embedding_dim))\n",
    "\n",
    "        # RNN weights\n",
    "        self.Wxh = nn.Parameter(torch.empty(embedding_dim, hidden_size))\n",
    "        self.Whh = nn.Parameter(torch.empty(hidden_size, hidden_size))\n",
    "        self.bh = nn.Parameter(torch.zeros(hidden_size)) # bias\n",
    "\n",
    "        # Output Layer\n",
    "        self.Why = nn.Parameter(torch.empty(hidden_size, vocab_size))\n",
    "        self.by = nn.Parameter(torch.zeros(vocab_size)) # bias\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        bound = 1.0 / math.sqrt(self.hidden_size)\n",
    "        nn.init.uniform_(self.E, -bound, bound)\n",
    "        nn.init.uniform_(self.Wxh, -bound, bound)\n",
    "        nn.init.uniform_(self.Whh, -bound, bound)\n",
    "        nn.init.uniform_(self.Why, -bound, bound)\n",
    "   \n",
    "    def forward(self, token_ids):\n",
    "        \"\"\"\n",
    "        token_ids: (T,)  one sentence\n",
    "        \"\"\"\n",
    "        h = torch.zeros(self.hidden_size) # h0\n",
    "\n",
    "        for t in range(len(token_ids)):\n",
    "            x_t = self.E[token_ids[t]]\n",
    "\n",
    "            preact = x_t @ self.Wxh + h @ self.Whh + self.bh\n",
    "            h = torch.tanh(preact)\n",
    "\n",
    "        logist = h @ self.Why + self.by\n",
    "        return logist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ef101e",
   "metadata": {},
   "source": [
    "# Training (No batch, one by one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffaad86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 | Loss: 0.3788 | Acc: 1.00\n",
      "Epoch 100 | Loss: 0.0928 | Acc: 1.00\n"
     ]
    }
   ],
   "source": [
    "model = SimpleRNN(len(vocab))\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.01)\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    for q, a in DATA:\n",
    "        q_ids = torch.tensor(encode(tokenize(q)), dtype=torch.long)\n",
    "        y_id  = torch.tensor([stoi[tokenize(a)[0]]], dtype=torch.long)\n",
    "    \n",
    "        logits = model(q_ids)\n",
    "        loss = loss_fn(logits.unsqueeze(0), y_id)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        pred = logits.argmax().item()\n",
    "        if pred == y_id.item():\n",
    "            correct += 1\n",
    "    \n",
    "    if (epoch+1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch+1} | Loss: {total_loss:.4f} | Acc: {correct/len(DATA):.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4f11084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: paris\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# 5) Test\n",
    "# -----------------------\n",
    "test_q = \"capital of france\"\n",
    "test_ids = torch.tensor(encode(tokenize(test_q)), dtype=torch.long)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(test_ids)\n",
    "    pred_id = logits.argmax().item()\n",
    "    print(\"Predicted:\", itos[pred_id])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
