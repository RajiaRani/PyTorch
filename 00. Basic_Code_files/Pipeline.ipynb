{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5b30d35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f4a6fd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.rand([5,10])\n",
    "# ✅ binary targets must be 0/1 and match output shape (5,1)\n",
    "# y = torch.tensor([[1.0],[0.0],[1.0],[0.0],[1.0]])   # shape (5,1)\n",
    "y = torch.ones(5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c4a42639",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySimpleNN():\n",
    "   def __init__(self, input):\n",
    "      \n",
    "   # define the weights\n",
    "    self.weights = torch.rand(input.shape[1], 1,dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "  #. bias\n",
    "    self.bias = torch.zeros(1, dtype=torch.float32,  requires_grad=True)\n",
    "\n",
    "\n",
    "   def forward(self, input):\n",
    "    z = torch.matmul(input, self.weights) + self.bias\n",
    "    y_pred = torch.sigmoid(z)\n",
    "    return y_pred\n",
    "\n",
    "   def calculate_loss(self, y, y_pred):\n",
    "     epsilon = 1e-7\n",
    "     y_pred = torch.clamp(y_pred, epsilon, 1-epsilon)\n",
    "     loss = -(y * torch.log(y_pred) + ((1 - y) * torch.log(1 - y_pred))).mean()\n",
    "     return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "48059af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "283f4db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4303],\n",
      "        [0.1003],\n",
      "        [0.0136],\n",
      "        [0.1907],\n",
      "        [0.2870],\n",
      "        [0.8036],\n",
      "        [0.8616],\n",
      "        [0.6483],\n",
      "        [0.3007],\n",
      "        [0.8278]], requires_grad=True)\n",
      "tensor([0.], requires_grad=True)\n",
      "tensor([[0.8681],\n",
      "        [0.9116],\n",
      "        [0.9572],\n",
      "        [0.9416],\n",
      "        [0.9197]], grad_fn=<SigmoidBackward0>)\n",
      "epoch 0 loss: 0.08432845771312714\n",
      "tensor([[0.8715],\n",
      "        [0.9139],\n",
      "        [0.9587],\n",
      "        [0.9436],\n",
      "        [0.9217]], grad_fn=<SigmoidBackward0>)\n",
      "epoch 1 loss: 0.08188091218471527\n",
      "tensor([[0.8747],\n",
      "        [0.9160],\n",
      "        [0.9602],\n",
      "        [0.9454],\n",
      "        [0.9236]], grad_fn=<SigmoidBackward0>)\n",
      "epoch 2 loss: 0.07956887781620026\n",
      "tensor([[0.8777],\n",
      "        [0.9181],\n",
      "        [0.9616],\n",
      "        [0.9472],\n",
      "        [0.9254]], grad_fn=<SigmoidBackward0>)\n",
      "epoch 3 loss: 0.07738165557384491\n",
      "tensor([[0.8806],\n",
      "        [0.9201],\n",
      "        [0.9629],\n",
      "        [0.9488],\n",
      "        [0.9271]], grad_fn=<SigmoidBackward0>)\n",
      "epoch 4 loss: 0.07530953735113144\n",
      "tensor([[0.8833],\n",
      "        [0.9220],\n",
      "        [0.9641],\n",
      "        [0.9504],\n",
      "        [0.9287]], grad_fn=<SigmoidBackward0>)\n",
      "epoch 5 loss: 0.07334376871585846\n",
      "tensor([[0.8860],\n",
      "        [0.9238],\n",
      "        [0.9652],\n",
      "        [0.9518],\n",
      "        [0.9303]], grad_fn=<SigmoidBackward0>)\n",
      "epoch 6 loss: 0.0714765265583992\n",
      "tensor([[0.8885],\n",
      "        [0.9255],\n",
      "        [0.9663],\n",
      "        [0.9532],\n",
      "        [0.9318]], grad_fn=<SigmoidBackward0>)\n",
      "epoch 7 loss: 0.06970066577196121\n",
      "tensor([[0.8909],\n",
      "        [0.9271],\n",
      "        [0.9674],\n",
      "        [0.9546],\n",
      "        [0.9332]], grad_fn=<SigmoidBackward0>)\n",
      "epoch 8 loss: 0.06800965964794159\n",
      "tensor([[0.8932],\n",
      "        [0.9287],\n",
      "        [0.9683],\n",
      "        [0.9558],\n",
      "        [0.9346]], grad_fn=<SigmoidBackward0>)\n",
      "epoch 9 loss: 0.06639771163463593\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "model = MySimpleNN(input)\n",
    "print(model.weights)\n",
    "print(model.bias)\n",
    "\n",
    "for epoch in range(epochs):\n",
    " # forward pss\n",
    "  forward = model.forward(input)\n",
    "  print(forward)\n",
    "\n",
    "  # backward\n",
    "  loss = model.calculate_loss(y, forward)\n",
    "  print(f\"epoch {epoch} loss:\", loss.item())\n",
    "\n",
    "  # forward\n",
    "  loss.backward()\n",
    "  # updates the weights and bias - \n",
    "  with torch.no_grad():\n",
    "   model.weights -= lr*model.weights.grad\n",
    "   model.bias -= lr*model.bias.grad\n",
    "\n",
    "   model.weights.grad.zero_()\n",
    "   model.bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6cffc3",
   "metadata": {},
   "source": [
    "# CODE 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "872d5b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand([5,5])\n",
    "y = torch.tensor(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5f0c2782",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBasicNN():\n",
    "    def __init__(self, x):\n",
    "        self.w = torch.ones(x.shape[1], 4, dtype=torch.float32, requires_grad=True)\n",
    "        self.b = torch.zeros(1, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "    # for forward pss\n",
    "    def forward(self, x):\n",
    "        z = torch.matmul(x,self.w) + self.b\n",
    "        y_pred = (torch.sigmoid(z)).mean()\n",
    "        return y_pred\n",
    "    \n",
    "    def loss_function(self, y, y_pred):\n",
    "        loss = torch.binary_cross_entropy_with_logits(y, y_pred)\n",
    "        return y_pred\n",
    "    \n",
    "    # ✅ evaluation\n",
    "    def evaluate(self, x, y, threshold=0.5):\n",
    "        with torch.no_grad():\n",
    "            y_prob = self.forward(x)\n",
    "            y_hat = (y_prob>=threshold).float()\n",
    "            accuracy = (y_hat == y).float()\n",
    "            return y_hat, y_prob, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d59b089e",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6232f046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8982, grad_fn=<MeanBackward0>)\n",
      "tensor(0.8982, grad_fn=<MeanBackward0>)\n",
      "tensor([[1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.9999, 0.9999, 0.9999, 0.9999],\n",
      "        [0.9999, 0.9999, 0.9999, 0.9999],\n",
      "        [0.9999, 0.9999, 0.9999, 0.9999],\n",
      "        [0.9999, 0.9999, 0.9999, 0.9999]], requires_grad=True)\n",
      "tensor([-0.0009], requires_grad=True)\n",
      "tensor(0.8981, grad_fn=<MeanBackward0>)\n",
      "tensor(0.8981, grad_fn=<MeanBackward0>)\n",
      "tensor([[0.9999, 0.9999, 0.9999, 0.9999],\n",
      "        [0.9999, 0.9999, 0.9999, 0.9999],\n",
      "        [0.9998, 0.9998, 0.9998, 0.9998],\n",
      "        [0.9998, 0.9998, 0.9998, 0.9998],\n",
      "        [0.9998, 0.9998, 0.9998, 0.9998]], requires_grad=True)\n",
      "tensor([-0.0017], requires_grad=True)\n",
      "tensor(0.8980, grad_fn=<MeanBackward0>)\n",
      "tensor(0.8980, grad_fn=<MeanBackward0>)\n",
      "tensor([[0.9999, 0.9999, 0.9999, 0.9999],\n",
      "        [0.9998, 0.9998, 0.9998, 0.9998],\n",
      "        [0.9997, 0.9997, 0.9997, 0.9997],\n",
      "        [0.9997, 0.9997, 0.9997, 0.9997],\n",
      "        [0.9997, 0.9997, 0.9997, 0.9997]], requires_grad=True)\n",
      "tensor([-0.0026], requires_grad=True)\n",
      "tensor(0.8980, grad_fn=<MeanBackward0>)\n",
      "tensor(0.8980, grad_fn=<MeanBackward0>)\n",
      "tensor([[0.9998, 0.9998, 0.9998, 0.9998],\n",
      "        [0.9997, 0.9997, 0.9997, 0.9997],\n",
      "        [0.9996, 0.9996, 0.9996, 0.9996],\n",
      "        [0.9996, 0.9996, 0.9996, 0.9996],\n",
      "        [0.9997, 0.9997, 0.9997, 0.9997]], requires_grad=True)\n",
      "tensor([-0.0034], requires_grad=True)\n",
      "tensor(0.8979, grad_fn=<MeanBackward0>)\n",
      "tensor(0.8979, grad_fn=<MeanBackward0>)\n",
      "tensor([[0.9998, 0.9998, 0.9998, 0.9998],\n",
      "        [0.9997, 0.9997, 0.9997, 0.9997],\n",
      "        [0.9995, 0.9995, 0.9995, 0.9995],\n",
      "        [0.9994, 0.9994, 0.9994, 0.9994],\n",
      "        [0.9996, 0.9996, 0.9996, 0.9996]], requires_grad=True)\n",
      "tensor([-0.0043], requires_grad=True)\n",
      "tensor(0.8978, grad_fn=<MeanBackward0>)\n",
      "tensor(0.8978, grad_fn=<MeanBackward0>)\n",
      "tensor([[0.9997, 0.9997, 0.9997, 0.9997],\n",
      "        [0.9996, 0.9996, 0.9996, 0.9996],\n",
      "        [0.9994, 0.9994, 0.9994, 0.9994],\n",
      "        [0.9993, 0.9993, 0.9993, 0.9993],\n",
      "        [0.9995, 0.9995, 0.9995, 0.9995]], requires_grad=True)\n",
      "tensor([-0.0052], requires_grad=True)\n",
      "tensor(0.8977, grad_fn=<MeanBackward0>)\n",
      "tensor(0.8977, grad_fn=<MeanBackward0>)\n",
      "tensor([[0.9997, 0.9997, 0.9997, 0.9997],\n",
      "        [0.9995, 0.9995, 0.9995, 0.9995],\n",
      "        [0.9993, 0.9993, 0.9993, 0.9993],\n",
      "        [0.9992, 0.9992, 0.9992, 0.9992],\n",
      "        [0.9994, 0.9994, 0.9994, 0.9994]], requires_grad=True)\n",
      "tensor([-0.0060], requires_grad=True)\n",
      "tensor(0.8976, grad_fn=<MeanBackward0>)\n",
      "tensor(0.8976, grad_fn=<MeanBackward0>)\n",
      "tensor([[0.9996, 0.9996, 0.9996, 0.9996],\n",
      "        [0.9995, 0.9995, 0.9995, 0.9995],\n",
      "        [0.9992, 0.9992, 0.9992, 0.9992],\n",
      "        [0.9991, 0.9991, 0.9991, 0.9991],\n",
      "        [0.9993, 0.9993, 0.9993, 0.9993]], requires_grad=True)\n",
      "tensor([-0.0069], requires_grad=True)\n",
      "tensor(0.8975, grad_fn=<MeanBackward0>)\n",
      "tensor(0.8975, grad_fn=<MeanBackward0>)\n",
      "tensor([[0.9996, 0.9996, 0.9996, 0.9996],\n",
      "        [0.9994, 0.9994, 0.9994, 0.9994],\n",
      "        [0.9991, 0.9991, 0.9991, 0.9991],\n",
      "        [0.9990, 0.9990, 0.9990, 0.9990],\n",
      "        [0.9992, 0.9992, 0.9992, 0.9992]], requires_grad=True)\n",
      "tensor([-0.0077], requires_grad=True)\n",
      "tensor(0.8974, grad_fn=<MeanBackward0>)\n",
      "tensor(0.8974, grad_fn=<MeanBackward0>)\n",
      "tensor([[0.9995, 0.9995, 0.9995, 0.9995],\n",
      "        [0.9993, 0.9993, 0.9993, 0.9993],\n",
      "        [0.9990, 0.9990, 0.9990, 0.9990],\n",
      "        [0.9989, 0.9989, 0.9989, 0.9989],\n",
      "        [0.9992, 0.9992, 0.9992, 0.9992]], requires_grad=True)\n",
      "tensor([-0.0086], requires_grad=True)\n",
      "Evaluation:\n",
      "probability: 1.0\n",
      "predicted label: 0\n",
      "true label: 1\n",
      "accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "model = MyBasicNN(x)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  # forward\n",
    "  y_pred = model.forward(x)\n",
    "  print(y_pred)\n",
    "  # loss\n",
    "  loss = model.loss_function(y, y_pred)\n",
    "  print(loss)\n",
    "\n",
    "  # backward\n",
    "  loss.backward()\n",
    "\n",
    "# update the weights and baises\n",
    "  with torch.no_grad():\n",
    "    model.w -=  learning_rate*model.w.grad\n",
    "    model.b -= learning_rate*model.b.grad\n",
    "    print(model.w)\n",
    "    print(model.b)\n",
    "\n",
    "    model.w.grad.zero_()\n",
    "    model.b.grad.zero_()\n",
    "\n",
    "# after training loop\n",
    "y_prob, y_hat, acc = model.evaluate(x, y)\n",
    "\n",
    "print(\"Evaluation:\")\n",
    "print(\"probability:\", y_prob.item())\n",
    "print(\"predicted label:\", int(y_hat.item()))\n",
    "print(\"true label:\", int(y.item()))\n",
    "print(\"accuracy:\", float(acc.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5c7afc",
   "metadata": {},
   "source": [
    "# code 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "cdea071b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchviz in /Users/abroadhub/Library/Python/3.9/lib/python/site-packages (0.0.3)\n",
      "Requirement already satisfied: torch in /Users/abroadhub/Library/Python/3.9/lib/python/site-packages (from torchviz) (2.8.0)\n",
      "Requirement already satisfied: graphviz in /Users/abroadhub/Library/Python/3.9/lib/python/site-packages (from torchviz) (0.21)\n",
      "Requirement already satisfied: filelock in /Users/abroadhub/Library/Python/3.9/lib/python/site-packages (from torch->torchviz) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/abroadhub/Library/Python/3.9/lib/python/site-packages (from torch->torchviz) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/abroadhub/Library/Python/3.9/lib/python/site-packages (from torch->torchviz) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/abroadhub/Library/Python/3.9/lib/python/site-packages (from torch->torchviz) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/abroadhub/Library/Python/3.9/lib/python/site-packages (from torch->torchviz) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/abroadhub/Library/Python/3.9/lib/python/site-packages (from torch->torchviz) (2025.10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/abroadhub/Library/Python/3.9/lib/python/site-packages (from sympy>=1.13.3->torch->torchviz) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/abroadhub/Library/Python/3.9/lib/python/site-packages (from jinja2->torch->torchviz) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torchviz  # -- run on terminal\n",
    "\n",
    "from torchviz import make_dot\n",
    "\n",
    "input = torch.tensor([0.1, 0.2, 0.3, 0.4]) # 4 inputs\n",
    "y = torch.tensor(1.0)\n",
    "\n",
    "class MyNeuralNetworks():\n",
    "    def __init__(self, input):\n",
    "        self.w1 = torch.rand(input.shape[0], 3, dtype=torch.float32, requires_grad=True)\n",
    "        self.b1 = torch.zeros(3, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "        # for 2nd layer\n",
    "        self.w2 = torch.rand(3, 2, dtype=torch.float32, requires_grad=True)\n",
    "        self.b2 = torch.zeros(2, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "        # final layer with 1 neuron\n",
    "        self.w3 = torch.rand(2, 1, dtype=torch.float32, requires_grad=True)\n",
    "        self.b3 = torch.zeros(1, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "    def forward(self, input):\n",
    "        z1 = torch.matmul(input, self.w1)+ self.b1\n",
    "        a1 = torch.relu(z1)\n",
    "\n",
    "        z2 = torch.matmul(a1, self.w2) + self.b2\n",
    "        a2 = torch.relu(z2)\n",
    "\n",
    "        z3 = torch.matmul(a2, self.w3) + self.b3\n",
    "        y_pred = torch.sigmoid(z3)\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    # loss\n",
    "    def loss_function(self, y, y_pred):\n",
    "        loss = torch.binary_cross_entropy_with_logits(y, y_pred)\n",
    "        return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "78d2eeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0652b93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor([0.5120], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss: tensor([0.5099], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss: tensor([0.5058], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss: tensor([0.4997], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss: tensor([0.4917], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss: tensor([0.4821], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss: tensor([0.4709], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss: tensor([0.4584], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss: tensor([0.4450], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss: tensor([0.4310], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = MyNeuralNetworks(input)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # forward pass\n",
    "    y_pred = model.forward(input)\n",
    "    # print(\"forward pass \" , y_pred)\n",
    "\n",
    "    # loss\n",
    "    loss = model.loss_function(y, y_pred)\n",
    "    print(\"loss:\", loss)\n",
    "\n",
    "    # back propagation\n",
    "    loss.backward()\n",
    "\n",
    "    # update the learnable parameters\n",
    "    with torch.no_grad():\n",
    "     model.w1 -= lr*model.w1.grad\n",
    "     model.b1 -= lr*model.b1.grad\n",
    "     model.w2 -= lr*model.w2.grad\n",
    "     model.b2 -= lr*model.b2.grad\n",
    "     model.w3 -= lr*model.w3.grad\n",
    "     model.b3 -= lr*model.b3.grad\n",
    "\n",
    "     model.w1.grad.zero_\n",
    "     model.w2.grad.zero_\n",
    "     model.w3.grad.zero_\n",
    "     model.b1.grad.zero_ \n",
    "     model.b2.grad.zero_\n",
    "     model.b3.grad.zero_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83588234",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m make_dot(loss, params\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m----> 2\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw1\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mw1\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m\"\u001b[39m: b1,\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw2\u001b[39m\u001b[38;5;124m\"\u001b[39m: w2, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb2\u001b[39m\u001b[38;5;124m\"\u001b[39m: b2,\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw3\u001b[39m\u001b[38;5;124m\"\u001b[39m: w3, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb3\u001b[39m\u001b[38;5;124m\"\u001b[39m: b3\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m })\n",
      "\u001b[0;31mNameError\u001b[0m: name 'w1' is not defined"
     ]
    }
   ],
   "source": [
    "make_dot(loss, params={\n",
    "        \"w1\": w1, \"b1\": b1,\n",
    "        \"w2\": w2, \"b2\": b2,\n",
    "        \"w3\": w3, \"b3\": b3\n",
    "\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
