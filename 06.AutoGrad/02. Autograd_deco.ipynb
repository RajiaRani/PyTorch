{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c674855",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eb0d619d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs = tensor([1., 1., 1., 1., 1.])\n",
      "output = tensor([0., 0., 0.])\n",
      "weight = tensor([[0.6337, 0.3109, 0.4552],\n",
      "        [0.0647, 0.3590, 0.9581],\n",
      "        [0.4798, 0.6914, 0.1591],\n",
      "        [0.9993, 0.0491, 0.4362],\n",
      "        [0.4417, 0.4780, 0.4679]], requires_grad=True)\n",
      "bias = tensor([0.4493, 0.2337, 0.3934], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# inputs\n",
    "x = torch.ones(5)\n",
    "# output\n",
    "y = torch.zeros(3)\n",
    "\n",
    "# weights\n",
    "w = torch.rand(5, 3, requires_grad=True)\n",
    "\n",
    "# bias\n",
    "b = torch.rand(3, requires_grad=True)\n",
    "\n",
    "print(\"inputs =\" , x)\n",
    "print(\"output =\", y)\n",
    "print(\"weight =\", w)\n",
    "print(\"bias =\", b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be25c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.0685, 2.1221, 2.8699], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# find z \n",
    "z = torch.matmul(x, w) + b\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "12130f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9556, 0.8930, 0.9463], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# apply activation function\n",
    "y_pred = torch.sigmoid(z)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2c0d97dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# find loss\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(y, y_pred)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "04940c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x1115f4760>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x1113ef430>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Gradient function for z = {z.grad_fn}\")\n",
    "print(f\"Gradient function for loss = {loss.grad_fn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b3ccef49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([-0., -0., -0.])\n"
     ]
    }
   ],
   "source": [
    "# Apply backward\n",
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6549f271",
   "metadata": {},
   "source": [
    "**We can only obtain the grad properties for the leaf nodes of the computational graph, which have requires_grad property set to True. For all other nodes in our graph, gradients will not be available.**\n",
    "\n",
    "**We can only perform gradient calculations using backward once on a given graph, for performance reasons. If we need to do several backward calls on the same graph, we need to pass retain_graph=True to the backward call.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b89a29f",
   "metadata": {},
   "source": [
    "# Disabling Gradient Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc71320",
   "metadata": {},
   "source": [
    "By default, all tensors with requires_grad=True are tracking their computational history and support gradient computation. However, there are some cases when we do not need to do that, for example, when we have trained the model and just want to apply it to some input data, i.e. we only want to do forward computations through the network. We can stop tracking computations by surrounding our computation code with torch.no_grad() block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1ae3d1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x,w) + b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x,w)+ b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555e9018",
   "metadata": {},
   "source": [
    "**Another way to achieve the same result is to use the detach() method on the tensor:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "58b803b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x,w)+b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc7b1f6",
   "metadata": {},
   "source": [
    "# There are reasons you might want to disable gradient tracking:\n",
    "*To mark some parameters in your neural network as frozen parameters.*\n",
    "\n",
    "*To speed up computations when you are only doing forward pass, because computations on tensors that do not track gradients would be more efficient.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
